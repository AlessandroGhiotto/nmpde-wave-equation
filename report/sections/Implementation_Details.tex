\section{Implementation Details}

This section describes the software architecture and organization of the numerical solver for the wave equation.

\subsection{Overall Architecture}

The software follows an object-oriented design with a modular structure. The implementation is built with C++17 and uses the \texttt{deal.II} finite element library for spatial discretization, combined with \texttt{Trilinos} for distributed linear algebra operations. The code is designed to support both sequential and parallel (MPI) execution.

\subsubsection{Class Hierarchy}

The core architecture revolves around a base class and derived implementations:

\begin{itemize}
    \item \textbf{WaveEquationBase}: An abstract base class that encapsulates common functionality for solving the wave equation. This includes mesh management, degree of freedom handling, assembly routines, and I/O operations. The class defines the interface through a pure virtual method \texttt{run()} that derived classes must implement.
    
    \item \textbf{WaveNewmark}: A concrete implementation using the Newmark time integration method. This class inherits from \texttt{WaveEquationBase} and implements time-stepping via the Newmark scheme with parameters $\gamma$ and $\beta$. It manages additional data structures specific to the acceleration discretization.
    
    \item \textbf{WaveTheta}: A concrete implementation using the theta method for time integration. This class also inherits from \texttt{WaveEquationBase} and implements time-stepping via a general theta-method scheme parameterized by $\theta \in [0,1]$.
\end{itemize}

\subsection{Directory Structure}

The project is organized as follows:

\begin{itemize}
    \item \textbf{include/}: Contains C++ header files defining the class interfaces. This includes \texttt{WaveEquationBase.hpp}, \texttt{WaveNewmark.hpp}, \texttt{WaveTheta.hpp}, and \texttt{ParameterReader.hpp}.
    
    \item \textbf{src/}: Contains C++ implementation files. The directory holds the implementations of the classes declared in \texttt{include/}, as well as the main entry points \texttt{main-newmark.cpp} and \texttt{main-theta.cpp}.
    
    \item \textbf{parameters/}: Contains JSON configuration files that specify problem parameters (PDE coefficients, initial/boundary conditions, discretization parameters, etc.). Each JSON file defines a particular test case.
    
    \item \textbf{mesh/}: Contains mesh files in VTK format used for spatial discretization. Several pre-generated rectangular meshes of different refinement levels are available.
    
    \item \textbf{results/}: Output directory where solutions and analysis data are stored. Results are organized in subdirectories named after the test case and discretization parameters.
    
    \item \textbf{analysis/}: Contains Python scripts and Jupyter notebooks for post-processing, convergence analysis, and energy/dissipation studies.
    
    \item \textbf{report/}: Contains the \LaTeX{} source for the project documentation.
\end{itemize}

\subsection{Parameter System}

A key design feature is the flexible parameter input system. The \texttt{ParameterReader} class handles parsing of JSON configuration files, which specify:

\begin{itemize}
    \item \textbf{Domain and discretization}: Geometry bounds, number of elements, and polynomial degree $r$.
    
    \item \textbf{Temporal parameters}: Final time $T$, time step size $\Delta t$, and method-specific parameters ($\gamma, \beta$ for Newmark; $\theta$ for theta method).
    
    \item \textbf{Problem-specific data}: The wave speed $c(\mathbf{x})$, source term $f(t,\mathbf{x})$, initial displacement $u_0(\mathbf{x})$, initial velocity $v_0(\mathbf{x})$, boundary displacement $g(t,\mathbf{x})$, and boundary velocity $\partial_t g(t,\mathbf{x})$.
    
    \item \textbf{Output control}: Options for saving solutions, logging frequency, and data export settings.
\end{itemize}

These parameters are specified as mathematical expressions (function strings) parsed by \texttt{FunctionParser} from deal.II, allowing for arbitrary analytical expressions without recompilation.

\subsection{Execution Flow}

The execution of the solver follows this sequence:

\begin{enumerate}
    \item \textbf{Initialization}: A main program (\texttt{main-newmark.cpp} or \texttt{main-theta.cpp}) is invoked with a path to a JSON parameter file.
    
    \item \textbf{Parameter parsing}: The \texttt{ParameterReader} reads and validates the JSON file, extracting all problem parameters and function definitions.
    
    \item \textbf{Solver instantiation}: A \texttt{WaveNewmark} or \texttt{WaveTheta} object is created with the parsed parameters.
    
    \item \textbf{Setup phase}: The solver's \texttt{run()} method is called, which internally:
    \begin{itemize}
        \item Loads and refines the mesh according to specified parameters.
        \item Initializes the finite element space (using simplex elements of degree $r$).
        \item Sets up the degree of freedom handler and distributed numbering (for MPI parallelism).
        \item Assembles the mass matrix, stiffness matrix, and initial vectors.
    \end{itemize}
    
    \item \textbf{Time integration}: The time-stepping loop advances the solution from $t=0$ to $t=T$ using either Newmark or theta-method schemes.
    
    \item \textbf{Output and logging}: At specified intervals, energy, errors, and solution snapshots are computed and saved to the results directory. Parallel I/O is handled via deal.II's distributed output facilities.
\end{enumerate}

\subsection{Parallel Computing Strategy}

The code leverages MPI for distributed memory parallelism through deal.II's distributed triangulation and linear algebra. Key aspects include:

\begin{itemize}
    \item \textbf{Mesh distribution}: The mesh is partitioned across MPI processes using \texttt{fully\_distributed\_tria}.
    
    \item \textbf{DOF distribution}: Degrees of freedom are distributed such that each process owns a portion of unknowns, minimizing communication overhead.
    
    \item \textbf{Matrix assembly}: Local assembly contributions are computed on each process, then assembled into \texttt{TrilinosWrappers::SparseMatrix} objects for efficient distributed sparse linear algebra.
    
    \item \textbf{Linear algebra}: \texttt{Trilinos} provides sparse matrix storage and iterative solvers (Conjugate Gradient for symmetric systems) with distributed preconditioning (AMG).
\end{itemize}

The MPI communicator and rank information are propagated through the base class, ensuring consistent behavior across all derived implementations.

\subsection{Finite Element Discretization}

The spatial discretization employs the Galerkin FEM with the following features:

\begin{itemize}
    \item \textbf{Element type}: Simplex (triangular) elements with polynomial degree $r$ (specified per test case).
    
    \item \textbf{Quadrature}: Standard Gauss quadrature rules are used, with order chosen to exactly integrate polynomial products of the appropriate degree.
    
    \item \textbf{Matrix assembly}: Assembly routines compute contributions from cell and boundary integrals. The Dirichlet boundary condition (prescribed displacement $g(t,\mathbf{x})$) is imposed after assembly using penalty methods or matrix row modification.
    
    \item \textbf{Symmetry and definiteness}: Both the mass matrix $M$ and stiffness matrix $K$ are symmetric and positive definite, enabling the use of efficient CG solvers.
\end{itemize}

\subsection{Build System}

The project uses CMake for build configuration and compilation. The \texttt{CMakeLists.txt} file:

\begin{itemize}
    \item Detects and configures MPI and deal.II installations.
    
    \item Defines compilation targets: a static library \texttt{WaveEquationBase} and two executable targets for the Newmark and theta method solvers.
    
    \item Sets compiler flags for C++17 standard and enables useful warnings (\texttt{-Wfloat-conversion}, \texttt{-Wnon-virtual-dtor}, etc.).
    
    \item Supports both Release and Debug build types.
\end{itemize}

\subsection{Output and Data Analysis}

Simulation outputs are automatically organized in the \texttt{results/} directory with subdirectories named by problem identifier and parameters. Output includes:

\begin{itemize}
    \item \textbf{Solution snapshots}: VTK files containing the displacement field $u(\mathbf{x}, t)$ at regular intervals for visualization.
    
    \item \textbf{Energy data}: CSV files logging the total, kinetic, and potential energy over time for stability and conservation analysis.
    
    \item \textbf{Error metrics}: Computed difference between the numerical and exact solutions (when an exact solution is available).
    
    \item \textbf{Performance logs}: Timing information and solver iteration counts.
\end{itemize}

Post-processing is handled by Python scripts and Jupyter notebooks in the \texttt{analysis/} directory, which perform convergence studies, spectral analysis, and visualization.
