#!/bin/bash
#PBS -N scaling_final
#PBS -q cpu
## :host=cpu04 <- add what is free
#PBS -l select=1:ncpus=16:mpiprocs=16
#PBS -l walltime=48:00:00
#PBS -j oe
#PBS -o scaling_final.out

SOURCE_DIR="$PBS_O_WORKDIR"
PROJECT_ROOT="$SOURCE_DIR/.."

SCRATCH_ROOT="/scratch_local/nmpde-wave-equation_${PBS_JOBID}"
WORK_DIR="$SCRATCH_ROOT/scripts"
DEST_DIR="/home/u11177200/nmpde-wave-equation/scripts"

exec > "${SOURCE_DIR}/scaling_final.log" 2>&1

# Avoid accidental OpenMP oversubscription
export OMP_NUM_THREADS=1
export PYTHONUNBUFFERED=1

# Unique tag: hostname + short PBS job id (e.g. cpu04_12345)
# Allows launching on two nodes simultaneously without filename collisions.
JOB_TAG="$(hostname)_${PBS_JOBID%%.*}"
export JOB_TAG

echo "============================================="
echo "Job started on $(date)"
echo "Node: $(hostname)"
echo "JOB_TAG: $JOB_TAG"
echo "PBS_NODEFILE contents:"
cat "$PBS_NODEFILE"
echo "============================================="

rm -rf "$SCRATCH_ROOT"
mkdir -p "$SCRATCH_ROOT"

echo "Copying project to scratch..."
cp -r "$PROJECT_ROOT/scripts"    "$SCRATCH_ROOT/"
cp -r "$PROJECT_ROOT/parameters" "$SCRATCH_ROOT/"
cp -r "$PROJECT_ROOT/build"      "$SCRATCH_ROOT/"

cd "$WORK_DIR" || exit 1

# Run all processor counts sequentially on the SAME node.
# This guarantees identical hardware conditions for a fair comparison.
# Use multiple repeats so we can take the minimum (filters out contention).
REPEATS=3

# Guaranteed runs (within our 16-CPU allocation)
for NP in 1 2 4 8 16; do
    echo ""
    echo "============================================="
    echo "Running with nprocs=$NP at $(date)"
    echo "---------------------------------------------"
    echo "Binding report for nprocs=$NP:"
    mpirun -np "$NP" --bind-to core --map-by socket --report-bindings hostname 2>&1 | head -20
    echo "---------------------------------------------"
    echo "============================================="
    python3 -u scalability_single.py \
        --nprocs "$NP" \
        --repeats "$REPEATS" \
        --use-pbs-nodefile \
        --job-id "$JOB_TAG"
    echo "Finished nprocs=$NP at $(date)"
done

# Opportunistic run with 32 cores 
# just to get an idea
echo ""
echo "============================================="
echo "Attempting nprocs=32"
echo "============================================="
NP=32
# --oversubscribe allows more ranks than reserved slots
mpirun -np "$NP" --bind-to core --map-by socket --oversubscribe --report-bindings hostname 2>&1 | head -40
if [ $? -eq 0 ]; then
    echo "32-core binding succeeded, running benchmark..."
    python3 -u scalability_single.py \
        --nprocs "$NP" \
        --repeats "$REPEATS" \
        --use-pbs-nodefile \
        --mpi-arg=--oversubscribe \
        --job-id "$JOB_TAG"
    echo "Finished nprocs=$NP at $(date)"
else
    echo "32-core run failed (node likely busy). Skipping."
fi

# Merge all per-nprocs CSVs into one
echo ""
echo "Merging results..."
MERGED="scalability-results-all-${JOB_TAG}.csv"
FIRST_CSV="scalability-results-1-${JOB_TAG}.csv"
head -1 "$FIRST_CSV" > "$MERGED"
for NP in 1 2 4 8 16 32; do
    CSV="scalability-results-${NP}-${JOB_TAG}.csv"
    if [ -f "$CSV" ]; then
        tail -n +2 "$CSV" >> "$MERGED"
    fi
done

mkdir -p "$DEST_DIR"
cp -f scalability-results-*.csv "$DEST_DIR/"
cp -rf run-logs "$DEST_DIR/run-logs-${JOB_TAG}" || true

echo ""
echo "============================================="
echo "All done. Job finished on $(date)"
echo "============================================="
